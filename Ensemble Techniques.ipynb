{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "30f53c3e-db06-4c5a-a754-59dd300889e2",
      "metadata": {
        "id": "30f53c3e-db06-4c5a-a754-59dd300889e2"
      },
      "source": [
        "### Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dc16984-5490-420b-ace5-d38c93f6ae64",
      "metadata": {
        "id": "0dc16984-5490-420b-ace5-d38c93f6ae64"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "**1. Introduction to Ensemble Learning:**\n",
        "\n",
        "Ensemble Learning is a powerful machine learning technique where **multiple models (called base models or learners)** are combined to solve a problem and improve the overall performance. Instead of relying on a single model, ensemble methods use a group of models to make better and more accurate predictions.\n",
        "\n",
        "The idea is similar to asking several experts for advice rather than trusting just one. Even if individual models make errors, combining them can cancel out their weaknesses and give more reliable results.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Key Idea Behind Ensemble Learning:**\n",
        "\n",
        "The **main idea** is:\n",
        "> \"A group of weak learners can come together to form a strong learner.\"\n",
        "\n",
        "This means that even if individual models are not very accurate (called weak learners), their combination can result in a strong model with much better accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "**3. Why Ensemble Learning Works:**\n",
        "\n",
        "- **Reduces Overfitting:** Combining models helps prevent a single model from learning noise in the data.\n",
        "- **Reduces Bias or Variance:** Some ensemble methods focus on reducing bias (Boosting), others reduce variance (Bagging).\n",
        "- **Improves Accuracy:** Group decisions are often more accurate than individual ones.\n",
        "- **Increases Stability:** Results are more consistent and less sensitive to small data changes.\n",
        "\n",
        "---\n",
        "\n",
        "**4. Common Types of Ensemble Methods:**\n",
        "\n",
        "1. **Bagging (Bootstrap Aggregating):**  \n",
        "   - Multiple models are trained on different random samples of the data.  \n",
        "   - Final result is taken by majority voting (classification) or averaging (regression).  \n",
        "   - Example: Random Forest\n",
        "\n",
        "2. **Boosting:**  \n",
        "   - Models are trained one after another. Each new model tries to fix the mistakes of the previous ones.  \n",
        "   - Final result is a weighted combination of all models.  \n",
        "   - Example: AdaBoost, Gradient Boosting\n",
        "\n",
        "3. **Stacking:**  \n",
        "   - Multiple different models are trained in parallel.  \n",
        "   - Their outputs are combined using a **meta-model** that learns how best to combine their predictions.\n",
        "\n",
        "---\n",
        "\n",
        "**5. Real-Life Analogy:**\n",
        "\n",
        "Imagine trying to predict if it will rain tomorrow. You ask five weather apps.  \n",
        "- If 3 say \"Yes\" and 2 say \"No\", you go with the majority.  \n",
        "- This is like **Voting**, a form of Ensemble Learning.\n",
        "\n",
        "---\n",
        "\n",
        "**6. Summary:**\n",
        "\n",
        "Ensemble Learning improves performance by combining the strengths of multiple models. It leads to:\n",
        "- Better accuracy\n",
        "- More stable predictions\n",
        "- Reduced risk of overfitting or underfitting\n",
        "\n",
        "This technique is widely used in real-world machine learning problems like fraud detection, recommendation systems, and competitions like Kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3cf4a1c-3571-4d4e-a940-e1598d6e4478",
      "metadata": {
        "id": "d3cf4a1c-3571-4d4e-a940-e1598d6e4478"
      },
      "source": [
        "### Question 2: What is the difference between Bagging and Boosting?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20c7d987-2bd1-4a02-be04-2c9a138a55b7",
      "metadata": {
        "id": "20c7d987-2bd1-4a02-be04-2c9a138a55b7"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "**1. Introduction:**\n",
        "\n",
        "Bagging and Boosting are two popular ensemble learning techniques in machine learning. Both aim to improve model performance by combining multiple learners. However, they work in different ways and solve different problems.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Key Differences Between Bagging and Boosting:**\n",
        "\n",
        "| Feature                | Bagging                                     | Boosting                                       |\n",
        "|------------------------|----------------------------------------------|------------------------------------------------|\n",
        "| Goal                   | Reduce variance                             | Reduce bias                                    |\n",
        "| Data Sampling          | Random sampling with replacement            | Sequential learning with focus on errors      |\n",
        "| Model Training         | Independent models                          | Each model depends on the previous one        |\n",
        "| Error Handling         | Treat all errors equally                    | Focus more on misclassified points            |\n",
        "| Model Combination      | Simple majority vote or average             | Weighted average or weighted vote             |\n",
        "| Overfitting Risk       | Lower (Good for high-variance models)       | Higher (needs tuning to avoid overfitting)    |\n",
        "| Parallelization        | Easy to parallelize                         | Difficult to parallelize                      |\n",
        "| Examples               | Random Forest                               | AdaBoost, Gradient Boosting                   |\n",
        "\n",
        "---\n",
        "\n",
        "**3. Working Mechanism:**\n",
        "\n",
        "**Bagging (Bootstrap Aggregating):**\n",
        "- Multiple models are trained on different random subsets of the data.\n",
        "- Each model votes, and the final prediction is based on the majority vote or average.\n",
        "- It helps in reducing **variance** and avoids overfitting.\n",
        "- Example: Random Forest (uses decision trees).\n",
        "\n",
        "**Boosting:**\n",
        "- Models are trained **sequentially**. Each new model corrects the mistakes made by the previous models.\n",
        "- Final prediction is a **weighted sum** of all models.\n",
        "- It helps in reducing **bias**, but can lead to overfitting if not controlled.\n",
        "- Examples: AdaBoost, Gradient Boosting, XGBoost.\n",
        "\n",
        "---\n",
        "\n",
        "**4. Real-Life Analogy:**\n",
        "\n",
        "- **Bagging:** Like asking many students to solve a problem independently and then voting on the correct answer.\n",
        "- **Boosting:** Like one student learns from their mistakes, then the next student continues learning from those mistakes, improving each time.\n",
        "\n",
        "---\n",
        "\n",
        "**5. Summary:**\n",
        "\n",
        "- **Bagging** is useful for reducing variance. It is simple and less prone to overfitting.\n",
        "- **Boosting** is useful for reducing bias. It is more powerful but needs careful tuning.\n",
        "\n",
        "Both techniques are important and are widely used in machine learning problems to improve prediction accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "390932a6-6004-4d99-aa4a-7f498c75dea6",
      "metadata": {
        "id": "390932a6-6004-4d99-aa4a-7f498c75dea6"
      },
      "source": [
        "### Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fef94410-2eed-4bc5-b638-4532a8c52b30",
      "metadata": {
        "id": "fef94410-2eed-4bc5-b638-4532a8c52b30"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "**1. What is Bootstrap Sampling?**\n",
        "\n",
        "Bootstrap sampling is a **random sampling technique** where we:\n",
        "- Select data points **with replacement**\n",
        "- Create multiple datasets (called bootstrap samples) from the original dataset\n",
        "- Each bootstrap sample has the same number of records as the original dataset but may contain **duplicate rows**\n",
        "\n",
        "In simple words, we randomly pick records from the original dataset, and since we use \"with replacement\", the same record can be picked more than once.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Example of Bootstrap Sampling:**\n",
        "\n",
        "Suppose the original dataset has 5 rows:\n",
        "Data = [A, B, C, D, E]\n",
        "\n",
        "One possible bootstrap sample might be:\n",
        "[B, C, C, E, A]\n",
        "\n",
        "Another might be:\n",
        "[D, B, A, D, E]\n",
        "\n",
        "---\n",
        "\n",
        "**3. Role of Bootstrap Sampling in Bagging:**\n",
        "\n",
        "Bagging stands for **Bootstrap Aggregating**, and bootstrap sampling is the first step of this method. Here’s how it is used:\n",
        "\n",
        "1. **Multiple bootstrap samples** are created from the original dataset.\n",
        "2. A **separate model (like a decision tree)** is trained on each sample.\n",
        "3. All models make predictions, and the final result is obtained by:\n",
        "   - **Majority voting** (for classification)\n",
        "   - **Averaging** (for regression)\n",
        "\n",
        "---\n",
        "\n",
        "**4. Why Bootstrap Sampling is Important in Bagging:**\n",
        "\n",
        "- Ensures **diversity among models**: Each model sees a different version of the data.\n",
        "- Helps in **reducing variance**: Independent errors made by models cancel each other out.\n",
        "- Makes the ensemble **more stable and robust**.\n",
        "\n",
        "---\n",
        "\n",
        "**5. Role in Random Forest:**\n",
        "\n",
        "Random Forest is a Bagging-based algorithm that:\n",
        "- Uses **bootstrap sampling** to create multiple datasets\n",
        "- Trains a **decision tree** on each sample\n",
        "- Combines their predictions using majority voting or averaging\n",
        "- Also adds extra randomness by selecting **random subsets of features** during tree splitting\n",
        "\n",
        "So, bootstrap sampling is essential in Random Forest to:\n",
        "- Create diverse trees\n",
        "- Reduce overfitting\n",
        "- Improve generalization\n",
        "\n",
        "---\n",
        "\n",
        "**6. Summary:**\n",
        "\n",
        "Bootstrap sampling is a key technique in ensemble methods like Bagging and Random Forest. It allows multiple models to be trained on different versions of the same dataset, improving prediction performance by reducing variance and increasing model diversity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbe5389a-535a-48ec-a54d-30b340ccd433",
      "metadata": {
        "id": "fbe5389a-535a-48ec-a54d-30b340ccd433"
      },
      "source": [
        "### Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2e8e769-7e54-4045-aa15-c9b5705c2ea7",
      "metadata": {
        "id": "d2e8e769-7e54-4045-aa15-c9b5705c2ea7"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "**1. What are Out-of-Bag (OOB) Samples?**\n",
        "\n",
        "Out-of-Bag (OOB) samples are the data points **not included** in a specific bootstrap sample when performing bagging (bootstrap aggregating).\n",
        "\n",
        "- In bootstrap sampling, we select data **with replacement**.\n",
        "- About **63%** of the original data is usually selected in each bootstrap sample.\n",
        "- The remaining **37%** data points are **not selected** and are called **OOB samples**.\n",
        "\n",
        "**Important Point:**\n",
        "OOB samples are **different for each base model** in the ensemble.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Why OOB Samples are Useful:**\n",
        "\n",
        "- These samples act like a **validation set**.\n",
        "- Since they were not used during the training of that specific model, they can be used to **test the model’s accuracy**.\n",
        "- This helps us **avoid using a separate validation dataset**.\n",
        "\n",
        "---\n",
        "\n",
        "**3. What is OOB Score?**\n",
        "\n",
        "The **OOB Score** is the average accuracy (or error) of predictions made on the OOB samples.\n",
        "\n",
        "**Steps to calculate OOB Score:**\n",
        "1. Train each base model (e.g., decision tree) on its bootstrap sample.\n",
        "2. Use that model to predict the labels for its corresponding OOB samples.\n",
        "3. Collect the OOB predictions for all data points (from all trees that did not use those points).\n",
        "4. Compare OOB predictions with the true labels to calculate accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "**4. Use of OOB Score in Ensemble Models:**\n",
        "\n",
        "In ensemble models like **Random Forest**, the OOB Score is used as an **internal performance evaluation metric**:\n",
        "- It provides an unbiased estimate of model accuracy.\n",
        "- It helps in **hyperparameter tuning**.\n",
        "- It eliminates the need for a separate validation set.\n",
        "- It gives a quick idea of how well the model might perform on unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "**5. Advantages of OOB Evaluation:**\n",
        "\n",
        "- Saves computation time by using training data itself for validation.\n",
        "- Reduces data wastage by not needing to hold out a test set.\n",
        "- Gives nearly the same result as cross-validation.\n",
        "\n",
        "---\n",
        "\n",
        "**6. Summary:**\n",
        "\n",
        "- **OOB samples** are the data points left out of bootstrap samples.\n",
        "- **OOB score** is the accuracy measured on these samples.\n",
        "- OOB evaluation provides a reliable estimate of model performance without needing a separate test set.\n",
        "- It is especially useful in ensemble models like **Random Forest**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d26edba3-199d-46d9-80c4-118ef1c4dfb2",
      "metadata": {
        "id": "d26edba3-199d-46d9-80c4-118ef1c4dfb2"
      },
      "source": [
        "### Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "879f77be-9cef-4a1f-afc2-99b3ecd11108",
      "metadata": {
        "id": "879f77be-9cef-4a1f-afc2-99b3ecd11108"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "**1. Introduction:**\n",
        "\n",
        "Feature importance analysis helps us understand which features (input variables) are most useful for making predictions in a machine learning model.\n",
        "Both Decision Tree and Random Forest can measure feature importance, but they do it in different ways.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Feature Importance in a Single Decision Tree:**\n",
        "\n",
        "- A Decision Tree splits the dataset at nodes using features that give the **highest information gain** or **maximum reduction in impurity** (e.g., Gini Impurity, Entropy).\n",
        "- Feature importance is calculated by **measuring the total reduction in impurity** each feature provides, across all splits where it is used.\n",
        "- Formula:\n",
        "    Importance(feature) = Sum of (Impurity decrease × samples at node) / Total samples\n",
        "- The output is **relative importance scores** that sum to 1.\n",
        "- **Limitation:** A single tree can overfit to noise, so importance scores may not be stable.\n",
        "\n",
        "---\n",
        "\n",
        "**3. Feature Importance in a Random Forest:**\n",
        "\n",
        "- Random Forest is an ensemble of many Decision Trees.\n",
        "- Each tree is trained on a different **bootstrap sample** and uses **random feature subsets** at each split.\n",
        "- Feature importance is calculated as the **average impurity decrease** across all trees.\n",
        "- This process gives **more reliable and stable importance scores** because it reduces variance.\n",
        "- Random Forest can also use **permutation importance**, where feature values are shuffled to see how much the prediction accuracy drops.\n",
        "\n",
        "---\n",
        "\n",
        "**4. Key Differences Between Decision Tree and Random Forest Feature Importance:**\n",
        "\n",
        "| Aspect | Decision Tree | Random Forest |\n",
        "|--------|---------------|---------------|\n",
        "| Data Used | Entire dataset | Multiple bootstrap samples |\n",
        "| Feature Selection | All features considered at each split | Random subset of features at each split |\n",
        "| Stability | Can be unstable (sensitive to small changes in data) | More stable and reliable |\n",
        "| Bias Risk | High risk of overfitting | Lower risk due to averaging over many trees |\n",
        "| Output | Importance from one tree | Averaged importance from all trees |\n",
        "\n",
        "---\n",
        "\n",
        "**5. Summary:**\n",
        "\n",
        "- **Decision Tree:** Feature importance is based on impurity reduction in a single tree. It is fast but can be unstable.\n",
        "- **Random Forest:** Feature importance is averaged over many trees, making it more robust and less prone to overfitting.\n",
        "- In practice, Random Forest feature importance is preferred because it gives a more reliable ranking of features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "\n",
        "● Train a Random Forest Classifier\n",
        "\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "WhcOS9mH07S9"
      },
      "id": "WhcOS9mH07S9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b71c276f-74e5-4229-8a50-868ccd0b198f",
      "metadata": {
        "id": "b71c276f-74e5-4229-8a50-868ccd0b198f",
        "outputId": "cf2bb23e-9ecb-4bb8-af01-6a4036c634ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Classifier accuracy on test set: 0.9357\n",
            "\n",
            "Top 5 most important features:\n",
            "worst concave points    0.158955\n",
            "worst area              0.146962\n",
            "worst perimeter         0.085793\n",
            "worst radius            0.078952\n",
            "mean radius             0.077714\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the breast cancer dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)  # features\n",
        "y = pd.Series(data.target)  # labels\n",
        "\n",
        "# Split data into training and testing sets to evaluate the model\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Initialize and train the Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set and calculate accuracy\n",
        "y_pred = rf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Get feature importances from the trained model\n",
        "importances = rf.feature_importances_\n",
        "feature_importance_series = pd.Series(importances, index=X.columns)\n",
        "\n",
        "# Select top 5 most important features\n",
        "top5 = feature_importance_series.sort_values(ascending=False).head(5)\n",
        "\n",
        "# Print results\n",
        "print('Random Forest Classifier accuracy on test set:', round(acc, 4))\n",
        "print('\\nTop 5 most important features:')\n",
        "print(top5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "-ZU7YSGH0-FK"
      },
      "id": "-ZU7YSGH0-FK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f171fc4e-551a-4265-a11d-6671e13851b4",
      "metadata": {
        "id": "f171fc4e-551a-4265-a11d-6671e13851b4",
        "outputId": "c8bb95ff-7833-46d6-99a9-f00ff419789b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Single Decision Tree: 0.9333\n",
            "Accuracy of Bagging Classifier:   0.9333\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Single Decision Tree\n",
        "dtree = DecisionTreeClassifier(random_state=42)\n",
        "dtree.fit(X_train, y_train)\n",
        "dtree_pred = dtree.predict(X_test)\n",
        "dtree_acc = accuracy_score(y_test, dtree_pred)\n",
        "\n",
        "# Bagging Classifier with Decision Tree\n",
        "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "bagging_pred = bagging.predict(X_test)\n",
        "bagging_acc = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Accuracy of Single Decision Tree:\", round(dtree_acc, 4))\n",
        "print(\"Accuracy of Bagging Classifier:  \", round(bagging_acc, 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "● Train a Random Forest Classifier\n",
        "\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "\n",
        "● Print the best parameters and final accuracy\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "E-u7bZPy1DNi"
      },
      "id": "E-u7bZPy1DNi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ad2107d-1dc0-46f8-9300-306fdb8dd913",
      "metadata": {
        "id": "0ad2107d-1dc0-46f8-9300-306fdb8dd913",
        "outputId": "cc1778c5-ea30-470c-fe45-31472c4a9e27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'max_depth': 2, 'n_estimators': 10}\n",
            "Final Accuracy: 0.9111\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Create Random Forest classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'max_depth': [2, 4, 6, None]\n",
        "}\n",
        "\n",
        "# Use GridSearchCV to search best parameters\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best model and evaluate on test set\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Final Accuracy:\", round(final_accuracy, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "\n",
        "● Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "m1uLzbnL1HRm"
      },
      "id": "m1uLzbnL1HRm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7c9b7ca-f2d9-4da7-b572-0a96857940df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7c9b7ca-f2d9-4da7-b572-0a96857940df",
        "outputId": "0198cea6-dd4b-41df-e8e1-daa3ab11440e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error of Bagging Regressor: 0.2579\n",
            "Mean Squared Error of Random Forest Regressor: 0.2577\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "california = fetch_california_housing()\n",
        "X = california.data\n",
        "y = california.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Bagging Regressor\n",
        "bagging_reg = BaggingRegressor(n_estimators=50, random_state=42)\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "bagging_preds = bagging_reg.predict(X_test)\n",
        "bagging_mse = mean_squared_error(y_test, bagging_preds)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_preds = rf_reg.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_preds)\n",
        "\n",
        "# Compare the Mean Squared Errors\n",
        "print(\"Mean Squared Error of Bagging Regressor:\", round(bagging_mse, 4))\n",
        "print(\"Mean Squared Error of Random Forest Regressor:\", round(rf_mse, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "\n",
        "● Choose between Bagging or Boosting\n",
        "\n",
        "● Handle overfitting\n",
        "\n",
        "● Select base models\n",
        "\n",
        "● Evaluate performance using cross-validation\n",
        "\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "YIQFK-391L0P"
      },
      "id": "YIQFK-391L0P"
    },
    {
      "cell_type": "markdown",
      "id": "4780bc14-d045-4d5c-bba7-d6eb4fbb35ee",
      "metadata": {
        "id": "4780bc14-d045-4d5c-bba7-d6eb4fbb35ee"
      },
      "source": [
        "**Scenario:**\n",
        "You are working as a data scientist at a financial institution. The goal is to predict loan default using customer demographic and transaction history data.\n",
        "\n",
        "We will outline the approach to use ensemble learning effectively in this real-world task.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Choose Between Bagging or Boosting\n",
        "\n",
        "- **Bagging** (e.g., Random Forest): Best when base models are high-variance and overfit easily.\n",
        "- **Boosting** (e.g., Gradient Boosting, XGBoost): Best when the model needs to learn complex patterns and improve bias.\n",
        "\n",
        "**Strategy:**\n",
        "- Start with **Random Forest** (Bagging) for a strong baseline.\n",
        "- Then use **XGBoost** or **Gradient Boosting** to further improve accuracy.\n",
        "- Compare both using cross-validation.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Handle Overfitting\n",
        "\n",
        "- Use **cross-validation** to measure performance on unseen data.\n",
        "- In **Bagging**, limit max depth and number of trees.\n",
        "- In **Boosting**, tune learning rate and add regularization (like `min_child_weight`, `subsample`, etc).\n",
        "- Remove or combine highly correlated features.\n",
        "- Use **feature importance** to eliminate less relevant features.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Select Base Models\n",
        "\n",
        "- Use **Decision Trees** as base learners for both Bagging and Boosting.\n",
        "- They handle mixed types of data (numerical + categorical) well.\n",
        "- In advanced boosting frameworks (like XGBoost), decision trees are already optimized internally.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Evaluate Performance Using Cross-Validation\n",
        "\n",
        "- Use **Stratified K-Fold Cross-Validation** to preserve class imbalance.\n",
        "- Evaluate metrics like:\n",
        "  - **Accuracy**\n",
        "  - **Precision/Recall** (especially important for loan default class)\n",
        "  - **ROC AUC**\n",
        "- Use **GridSearchCV** or **RandomizedSearchCV** for tuning hyperparameters.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Justification: How Ensemble Learning Helps in Real-World Loan Default Prediction\n",
        "\n",
        "- Real-world financial data is **complex and noisy**.\n",
        "- Individual models may underfit or overfit.\n",
        "- **Ensemble methods combine predictions from many models**, reducing variance and bias.\n",
        "- Boosting **focuses on harder cases**, improving predictive power.\n",
        "- **Better predictions lead to better risk decisions**, reducing loan defaults and financial losses.\n",
        "- Provides **feature importance rankings**, useful for business teams to interpret decisions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89eec532-1045-4f4b-a751-0981a11ee230",
      "metadata": {
        "id": "89eec532-1045-4f4b-a751-0981a11ee230"
      },
      "outputs": [],
      "source": [
        "# Example Code Structure (Hypothetical)\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import roc_auc_score, make_scorer\n",
        "# from xgboost import XGBClassifier  # optionally\n",
        "\n",
        "# X, y = ... # your preprocessed data\n",
        "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# rf = RandomForestClassifier(max_depth=8, n_estimators=100, random_state=42)\n",
        "# rf_scores = cross_val_score(rf, X, y, cv=skf, scoring='roc_auc')\n",
        "\n",
        "# gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "# gb_scores = cross_val_score(gb, X, y, cv=skf, scoring='roc_auc')\n",
        "\n",
        "# print(\"Random Forest AUC:\", rf_scores.mean())\n",
        "# print(\"Gradient Boosting AUC:\", gb_scores.mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "# Create synthetic classification data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, random_state=42, weights=[0.7, 0.3])\n",
        "\n",
        "# Set up Stratified K-Fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Random Forest Model\n",
        "rf = RandomForestClassifier(max_depth=8, n_estimators=100, random_state=42)\n",
        "rf_scores = cross_val_score(rf, X, y, cv=skf, scoring='roc_auc')\n",
        "\n",
        "# Gradient Boosting Model\n",
        "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_scores = cross_val_score(gb, X, y, cv=skf, scoring='roc_auc')\n",
        "\n",
        "# Print results\n",
        "print(\"Random Forest AUC:\", round(rf_scores.mean(), 4))\n",
        "print(\"Gradient Boosting AUC:\", round(gb_scores.mean(), 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PYqJQAswlR6",
        "outputId": "e4eda23e-16cb-4ce5-fdf3-cc996dac612a"
      },
      "id": "-PYqJQAswlR6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest AUC: 0.9599\n",
            "Gradient Boosting AUC: 0.9602\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}